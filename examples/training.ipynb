{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import InputSpec\n",
    "import keras.backend as K\n",
    "from keras import regularizers\n",
    "from keras.layers import ZeroPadding2D, AveragePooling2D,BatchNormalization,Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import Merge, Input, Lambda, Layer, add\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.datasets import cifar10\n",
    "from keras_ode.layers import HamiltonianConv2D, ChannelZeroPadding\n",
    "\n",
    "(x_train, y_train) , (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/raghakot/keras-resnet/blob/master/cifar10.py\n",
    "# subtract mean and normalize\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train -= mean_image\n",
    "x_test -= mean_image\n",
    "x_train /= 128.\n",
    "x_test /= 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "built\n",
      "built\n",
      "built\n",
      "built\n",
      "built\n",
      "built\n"
     ]
    }
   ],
   "source": [
    "img_input = Input(shape=x_train.shape[1:])\n",
    "x = ZeroPadding2D(padding=(3, 3), name='conv1_pad')(img_input)\n",
    "x = Conv2D(16, (7, 7), strides=(1, 1), kernel_regularizer=regularizers.l2(.0001),\n",
    "           kernel_initializer='he_normal', padding='valid')(x)\n",
    "x = BatchNormalization(axis=-1)(x)\n",
    "x = Activation('relu')(x)\n",
    "    \n",
    "x = HamiltonianConv2D(filters=16, h=.15, unroll_length=4, kernel_regularizer=regularizers.l2(.0001),\n",
    "                    kernel_initializer='he_normal', kernel_size=(3,3),padding='same', activation='relu')(x)\n",
    "\n",
    "x = AveragePooling2D((2, 2))(x)\n",
    "x = ChannelZeroPadding(padding=4)(x)\n",
    "\n",
    "x = HamiltonianConv2D(filters=64, h=.15, unroll_length=4, kernel_regularizer=regularizers.l2(.0001),\n",
    "                    kernel_initializer='he_normal', kernel_size=(3,3),padding='same', activation='relu')(x)\n",
    "\n",
    "x = ChannelZeroPadding(padding=2)(x)\n",
    "\n",
    "x = HamiltonianConv2D(filters=128, h=.15, unroll_length=4, kernel_regularizer=regularizers.l2(.0001),\n",
    "                    kernel_initializer='he_normal', kernel_size=(3,3),padding='same', activation='relu')(x)\n",
    "\n",
    "x = ChannelZeroPadding(padding=2)(x)\n",
    "\n",
    "x = HamiltonianConv2D(filters=256, h=.15, unroll_length=4, kernel_regularizer=regularizers.l2(.0001),\n",
    "                    kernel_initializer='he_normal', kernel_size=(3,3),padding='same', activation='relu')(x)\n",
    "\n",
    "x = AveragePooling2D((2, 2))(x)\n",
    "x = ChannelZeroPadding(padding=2)(x)\n",
    "\n",
    "x = HamiltonianConv2D(filters=512, h=.15, unroll_length=4, kernel_regularizer=regularizers.l2(.0001),\n",
    "                    kernel_initializer='he_normal', kernel_size=(3,3),padding='same', activation='relu')(x)\n",
    "\n",
    "x = AveragePooling2D((2, 2))(x)\n",
    "x = ChannelZeroPadding(padding=2)(x)\n",
    "\n",
    "x = HamiltonianConv2D(filters=1024, h=.15, unroll_length=4, kernel_regularizer=regularizers.l2(.0001),\n",
    "                    kernel_initializer='he_normal', kernel_size=(3,3),padding='same', activation='relu')(x)\n",
    "\n",
    "x = AveragePooling2D((4, 4))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "out = Dense(10, kernel_regularizer=regularizers.l2(.0001), activation='softmax')(x)\n",
    "model = Model(img_input, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.1, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 38, 38, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 16)        2368      \n",
      "_________________________________________________________________\n",
      "bn_conv1 (BatchNormalization (None, 32, 32, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "hamiltonian_conv_7 (Hamilton (None, 32, 32, 16)        592       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_5 (Average (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "channel_zero_padding_6 (Chan (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "hamiltonian_conv_8 (Hamilton (None, 16, 16, 64)        9280      \n",
      "_________________________________________________________________\n",
      "channel_zero_padding_7 (Chan (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "hamiltonian_conv_9 (Hamilton (None, 16, 16, 128)       36992     \n",
      "_________________________________________________________________\n",
      "channel_zero_padding_8 (Chan (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "hamiltonian_conv_10 (Hamilto (None, 16, 16, 256)       147712    \n",
      "_________________________________________________________________\n",
      "average_pooling2d_6 (Average (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "channel_zero_padding_9 (Chan (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "hamiltonian_conv_11 (Hamilto (None, 8, 8, 512)         590336    \n",
      "_________________________________________________________________\n",
      "average_pooling2d_7 (Average (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "channel_zero_padding_10 (Cha (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "hamiltonian_conv_12 (Hamilto (None, 4, 4, 1024)        2360320   \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "fc1000 (Dense)               (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 3,157,914\n",
      "Trainable params: 3,157,882\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "    samplewise_center=False,  # set each sample mean to 0\n",
    "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "    samplewise_std_normalization=False,  # divide each input by its std\n",
    "    zca_whitening=False,  # apply ZCA whitening\n",
    "    rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_in_epoch = (x_train.shape[0] // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iterations = 64000*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = training_iterations / steps_in_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drops = [(training_iterations/3) / steps_in_epoch,\n",
    "         (2*training_iterations/3 )/ steps_in_epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[109, 218]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping,LearningRateScheduler\n",
    "    \n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 10.\n",
    "    \n",
    "    \n",
    "    if epoch < drops[0]:\n",
    "        power= 0\n",
    "    elif epoch < drops[1]:\n",
    "        power = 1\n",
    "    else:\n",
    "        power = 2\n",
    "    \n",
    "    \n",
    "    lrate = initial_lrate / np.power(drop, power)\n",
    "    print ('epoch:' + str(epoch) + ' learning rate:' + str(lrate)) # added\n",
    "    return lrate\n",
    "        \n",
    "        \n",
    "\n",
    "lr_scheduler = LearningRateScheduler(schedule=step_decay)\n",
    "csv_logger = CSVLogger('training_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bighd/venvs/base/lib/python2.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., verbose=1, validation_data=(array([[[..., steps_per_epoch=1562, epochs=327, callbacks=[<keras.ca..., max_queue_size=10)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 learning rate:0.1\n",
      "Epoch 1/327\n",
      "1562/1562 [==============================] - 284s 182ms/step - loss: 1.8719 - acc: 0.3557 - val_loss: 1.5520 - val_acc: 0.4827\n",
      "epoch:1 learning rate:0.1\n",
      "Epoch 2/327\n",
      "1562/1562 [==============================] - 282s 181ms/step - loss: 1.5287 - acc: 0.4948 - val_loss: 1.3262 - val_acc: 0.5790\n",
      "epoch:2 learning rate:0.1\n",
      "Epoch 3/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.3731 - acc: 0.5615 - val_loss: 1.3171 - val_acc: 0.6005\n",
      "epoch:3 learning rate:0.1\n",
      "Epoch 4/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.2890 - acc: 0.6038 - val_loss: 1.2283 - val_acc: 0.6254\n",
      "epoch:4 learning rate:0.1\n",
      "Epoch 5/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.2245 - acc: 0.6378 - val_loss: 1.2159 - val_acc: 0.6514\n",
      "epoch:5 learning rate:0.1\n",
      "Epoch 6/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.1836 - acc: 0.6593 - val_loss: 1.1436 - val_acc: 0.6818\n",
      "epoch:6 learning rate:0.1\n",
      "Epoch 7/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.1511 - acc: 0.6813 - val_loss: 1.1096 - val_acc: 0.7006\n",
      "epoch:7 learning rate:0.1\n",
      "Epoch 8/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.1253 - acc: 0.6934 - val_loss: 1.0731 - val_acc: 0.7207\n",
      "epoch:8 learning rate:0.1\n",
      "Epoch 9/327\n",
      "1562/1562 [==============================] - 280s 180ms/step - loss: 1.1114 - acc: 0.7055 - val_loss: 1.2385 - val_acc: 0.6681\n",
      "epoch:9 learning rate:0.1\n",
      "Epoch 10/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.0879 - acc: 0.7176 - val_loss: 1.0803 - val_acc: 0.7226\n",
      "epoch:10 learning rate:0.1\n",
      "Epoch 11/327\n",
      "1562/1562 [==============================] - 280s 180ms/step - loss: 1.0740 - acc: 0.7288 - val_loss: 1.0006 - val_acc: 0.7564\n",
      "epoch:11 learning rate:0.1\n",
      "Epoch 12/327\n",
      "1562/1562 [==============================] - 280s 180ms/step - loss: 1.0737 - acc: 0.7294 - val_loss: 1.1593 - val_acc: 0.7155\n",
      "epoch:12 learning rate:0.1\n",
      "Epoch 13/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.0677 - acc: 0.7349 - val_loss: 1.0537 - val_acc: 0.7482\n",
      "epoch:13 learning rate:0.1\n",
      "Epoch 14/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0535 - acc: 0.7432 - val_loss: 1.1048 - val_acc: 0.7325\n",
      "epoch:14 learning rate:0.1\n",
      "Epoch 15/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0433 - acc: 0.7502 - val_loss: 1.6065 - val_acc: 0.6126\n",
      "epoch:15 learning rate:0.1\n",
      "Epoch 16/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0337 - acc: 0.7528 - val_loss: 1.0107 - val_acc: 0.7650\n",
      "epoch:16 learning rate:0.1\n",
      "Epoch 17/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.0452 - acc: 0.7543 - val_loss: 1.0789 - val_acc: 0.7431\n",
      "epoch:17 learning rate:0.1\n",
      "Epoch 18/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.0309 - acc: 0.7593 - val_loss: 1.0079 - val_acc: 0.7670\n",
      "epoch:18 learning rate:0.1\n",
      "Epoch 19/327\n",
      "1562/1562 [==============================] - 280s 180ms/step - loss: 1.0283 - acc: 0.7607 - val_loss: 0.9959 - val_acc: 0.7736\n",
      "epoch:19 learning rate:0.1\n",
      "Epoch 20/327\n",
      "1562/1562 [==============================] - 280s 180ms/step - loss: 1.0177 - acc: 0.7671 - val_loss: 0.9746 - val_acc: 0.7834\n",
      "epoch:20 learning rate:0.1\n",
      "Epoch 21/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0254 - acc: 0.7662 - val_loss: 1.0871 - val_acc: 0.7442\n",
      "epoch:21 learning rate:0.1\n",
      "Epoch 22/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0230 - acc: 0.7685 - val_loss: 0.9527 - val_acc: 0.7936\n",
      "epoch:22 learning rate:0.1\n",
      "Epoch 23/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.0219 - acc: 0.7691 - val_loss: 1.0324 - val_acc: 0.7667\n",
      "epoch:23 learning rate:0.1\n",
      "Epoch 24/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.0275 - acc: 0.7694 - val_loss: 1.1005 - val_acc: 0.7498\n",
      "epoch:24 learning rate:0.1\n",
      "Epoch 25/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.0182 - acc: 0.7721 - val_loss: 1.0728 - val_acc: 0.7607\n",
      "epoch:25 learning rate:0.1\n",
      "Epoch 26/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0213 - acc: 0.7728 - val_loss: 1.0680 - val_acc: 0.7610\n",
      "epoch:26 learning rate:0.1\n",
      "Epoch 27/327\n",
      "1562/1562 [==============================] - 280s 180ms/step - loss: 1.0163 - acc: 0.7767 - val_loss: 1.0215 - val_acc: 0.7731\n",
      "epoch:27 learning rate:0.1\n",
      "Epoch 28/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0236 - acc: 0.7749 - val_loss: 1.0749 - val_acc: 0.7534\n",
      "epoch:28 learning rate:0.1\n",
      "Epoch 29/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0116 - acc: 0.7792 - val_loss: 1.0805 - val_acc: 0.7586\n",
      "epoch:29 learning rate:0.1\n",
      "Epoch 30/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.0035 - acc: 0.7815 - val_loss: 1.0561 - val_acc: 0.7608\n",
      "epoch:30 learning rate:0.1\n",
      "Epoch 31/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0046 - acc: 0.7831 - val_loss: 0.9631 - val_acc: 0.8006\n",
      "epoch:31 learning rate:0.1\n",
      "Epoch 32/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0064 - acc: 0.7815 - val_loss: 0.9582 - val_acc: 0.7981\n",
      "epoch:32 learning rate:0.1\n",
      "Epoch 33/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0030 - acc: 0.7853 - val_loss: 0.9713 - val_acc: 0.7953\n",
      "epoch:33 learning rate:0.1\n",
      "Epoch 34/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0023 - acc: 0.7873 - val_loss: 0.9828 - val_acc: 0.7982\n",
      "epoch:34 learning rate:0.1\n",
      "Epoch 35/327\n",
      "1562/1562 [==============================] - 280s 180ms/step - loss: 1.0050 - acc: 0.7874 - val_loss: 1.0395 - val_acc: 0.7757\n",
      "epoch:35 learning rate:0.1\n",
      "Epoch 36/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0036 - acc: 0.7865 - val_loss: 1.0192 - val_acc: 0.7802\n",
      "epoch:36 learning rate:0.1\n",
      "Epoch 37/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.0034 - acc: 0.7867 - val_loss: 1.0076 - val_acc: 0.7900\n",
      "epoch:37 learning rate:0.1\n",
      "Epoch 38/327\n",
      "1562/1562 [==============================] - 280s 180ms/step - loss: 1.0050 - acc: 0.7896 - val_loss: 1.2982 - val_acc: 0.6989\n",
      "epoch:38 learning rate:0.1\n",
      "Epoch 39/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.9948 - acc: 0.7921 - val_loss: 0.9981 - val_acc: 0.7954\n",
      "epoch:39 learning rate:0.1\n",
      "Epoch 40/327\n",
      "1562/1562 [==============================] - 280s 180ms/step - loss: 1.0085 - acc: 0.7864 - val_loss: 0.9780 - val_acc: 0.7996\n",
      "epoch:40 learning rate:0.1\n",
      "Epoch 41/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0092 - acc: 0.7884 - val_loss: 0.9484 - val_acc: 0.8134\n",
      "epoch:41 learning rate:0.1\n",
      "Epoch 42/327\n",
      "1562/1562 [==============================] - 280s 180ms/step - loss: 0.9991 - acc: 0.7907 - val_loss: 1.0103 - val_acc: 0.7899\n",
      "epoch:42 learning rate:0.1\n",
      "Epoch 43/327\n",
      "1562/1562 [==============================] - 280s 180ms/step - loss: 0.9882 - acc: 0.7939 - val_loss: 1.0035 - val_acc: 0.7855\n",
      "epoch:43 learning rate:0.1\n",
      "Epoch 44/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 1.0018 - acc: 0.7908 - val_loss: 0.9750 - val_acc: 0.7977\n",
      "epoch:44 learning rate:0.1\n",
      "Epoch 45/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 1.0032 - acc: 0.7917 - val_loss: 1.0785 - val_acc: 0.7648\n",
      "epoch:45 learning rate:0.1\n",
      "Epoch 46/327\n",
      "1562/1562 [==============================] - 280s 180ms/step - loss: 0.9958 - acc: 0.7917 - val_loss: 1.0607 - val_acc: 0.7775\n",
      "epoch:46 learning rate:0.1\n",
      "Epoch 47/327\n",
      "1562/1562 [==============================] - 280s 180ms/step - loss: 0.9993 - acc: 0.7938 - val_loss: 0.9532 - val_acc: 0.8113\n",
      "epoch:47 learning rate:0.1\n",
      "Epoch 48/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 0.9982 - acc: 0.7912 - val_loss: 0.9804 - val_acc: 0.8027\n",
      "epoch:48 learning rate:0.1\n",
      "Epoch 49/327\n",
      "1562/1562 [==============================] - 280s 179ms/step - loss: 0.9964 - acc: 0.7960 - val_loss: 1.0912 - val_acc: 0.7673\n",
      "epoch:49 learning rate:0.1\n",
      "Epoch 50/327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9951 - acc: 0.7944 - val_loss: 1.0296 - val_acc: 0.7915\n",
      "epoch:50 learning rate:0.1\n",
      "Epoch 51/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9939 - acc: 0.7959 - val_loss: 0.9812 - val_acc: 0.8002\n",
      "epoch:51 learning rate:0.1\n",
      "Epoch 52/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 1.0030 - acc: 0.7932 - val_loss: 1.1010 - val_acc: 0.7506\n",
      "epoch:52 learning rate:0.1\n",
      "Epoch 53/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9825 - acc: 0.7997 - val_loss: 1.0229 - val_acc: 0.7873\n",
      "epoch:53 learning rate:0.1\n",
      "Epoch 54/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9905 - acc: 0.7959 - val_loss: 1.0274 - val_acc: 0.7880\n",
      "epoch:54 learning rate:0.1\n",
      "Epoch 55/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9925 - acc: 0.7983 - val_loss: 1.1525 - val_acc: 0.7532\n",
      "epoch:55 learning rate:0.1\n",
      "Epoch 56/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9965 - acc: 0.7962 - val_loss: 0.9699 - val_acc: 0.8037\n",
      "epoch:56 learning rate:0.1\n",
      "Epoch 57/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9920 - acc: 0.7979 - val_loss: 0.9603 - val_acc: 0.8116\n",
      "epoch:57 learning rate:0.1\n",
      "Epoch 58/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9884 - acc: 0.8002 - val_loss: 1.0655 - val_acc: 0.7694\n",
      "epoch:58 learning rate:0.1\n",
      "Epoch 59/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9857 - acc: 0.8021 - val_loss: 0.9763 - val_acc: 0.8044\n",
      "epoch:59 learning rate:0.1\n",
      "Epoch 60/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9912 - acc: 0.8000 - val_loss: 0.9556 - val_acc: 0.8151\n",
      "epoch:60 learning rate:0.1\n",
      "Epoch 61/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9876 - acc: 0.8015 - val_loss: 0.9946 - val_acc: 0.7968\n",
      "epoch:61 learning rate:0.1\n",
      "Epoch 62/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9858 - acc: 0.8010 - val_loss: 1.1330 - val_acc: 0.7606\n",
      "epoch:62 learning rate:0.1\n",
      "Epoch 63/327\n",
      "1562/1562 [==============================] - 273s 174ms/step - loss: 0.9893 - acc: 0.8007 - val_loss: 1.0309 - val_acc: 0.7883\n",
      "epoch:63 learning rate:0.1\n",
      "Epoch 64/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9924 - acc: 0.7991 - val_loss: 1.0245 - val_acc: 0.7940\n",
      "epoch:64 learning rate:0.1\n",
      "Epoch 65/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9787 - acc: 0.8035 - val_loss: 1.0674 - val_acc: 0.7812\n",
      "epoch:65 learning rate:0.1\n",
      "Epoch 66/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9847 - acc: 0.8029 - val_loss: 1.1354 - val_acc: 0.7554\n",
      "epoch:66 learning rate:0.1\n",
      "Epoch 67/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9886 - acc: 0.8020 - val_loss: 1.0547 - val_acc: 0.7771\n",
      "epoch:67 learning rate:0.1\n",
      "Epoch 68/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9809 - acc: 0.8028 - val_loss: 1.0477 - val_acc: 0.7796\n",
      "epoch:68 learning rate:0.1\n",
      "Epoch 69/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9832 - acc: 0.8037 - val_loss: 1.0668 - val_acc: 0.7899\n",
      "epoch:69 learning rate:0.1\n",
      "Epoch 70/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9846 - acc: 0.8032 - val_loss: 1.0123 - val_acc: 0.7981\n",
      "epoch:70 learning rate:0.1\n",
      "Epoch 71/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9836 - acc: 0.8051 - val_loss: 1.0170 - val_acc: 0.7966\n",
      "epoch:71 learning rate:0.1\n",
      "Epoch 72/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9924 - acc: 0.8023 - val_loss: 0.9533 - val_acc: 0.8188\n",
      "epoch:72 learning rate:0.1\n",
      "Epoch 73/327\n",
      "1562/1562 [==============================] - 273s 174ms/step - loss: 0.9872 - acc: 0.8024 - val_loss: 1.0259 - val_acc: 0.7939\n",
      "epoch:73 learning rate:0.1\n",
      "Epoch 74/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9981 - acc: 0.7994 - val_loss: 0.9947 - val_acc: 0.8020\n",
      "epoch:74 learning rate:0.1\n",
      "Epoch 75/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9813 - acc: 0.8053 - val_loss: 1.0124 - val_acc: 0.8022\n",
      "epoch:75 learning rate:0.1\n",
      "Epoch 76/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9831 - acc: 0.8032 - val_loss: 1.0322 - val_acc: 0.7928\n",
      "epoch:76 learning rate:0.1\n",
      "Epoch 77/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9785 - acc: 0.8060 - val_loss: 0.9937 - val_acc: 0.8012\n",
      "epoch:77 learning rate:0.1\n",
      "Epoch 78/327\n",
      "1562/1562 [==============================] - 273s 174ms/step - loss: 0.9799 - acc: 0.8065 - val_loss: 1.0295 - val_acc: 0.7964\n",
      "epoch:78 learning rate:0.1\n",
      "Epoch 79/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9847 - acc: 0.8065 - val_loss: 1.0168 - val_acc: 0.7991\n",
      "epoch:79 learning rate:0.1\n",
      "Epoch 80/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9890 - acc: 0.8041 - val_loss: 1.0152 - val_acc: 0.7908\n",
      "epoch:80 learning rate:0.1\n",
      "Epoch 81/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9738 - acc: 0.8094 - val_loss: 0.9872 - val_acc: 0.8128\n",
      "epoch:81 learning rate:0.1\n",
      "Epoch 82/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9873 - acc: 0.8056 - val_loss: 1.0699 - val_acc: 0.7893\n",
      "epoch:82 learning rate:0.1\n",
      "Epoch 83/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9728 - acc: 0.8088 - val_loss: 0.9685 - val_acc: 0.8134\n",
      "epoch:83 learning rate:0.1\n",
      "Epoch 84/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9831 - acc: 0.8065 - val_loss: 1.1345 - val_acc: 0.7568\n",
      "epoch:84 learning rate:0.1\n",
      "Epoch 85/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9802 - acc: 0.8061 - val_loss: 0.9688 - val_acc: 0.8165\n",
      "epoch:85 learning rate:0.1\n",
      "Epoch 86/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9851 - acc: 0.8076 - val_loss: 0.9902 - val_acc: 0.8114\n",
      "epoch:86 learning rate:0.1\n",
      "Epoch 87/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9776 - acc: 0.8101 - val_loss: 1.0145 - val_acc: 0.8027\n",
      "epoch:87 learning rate:0.1\n",
      "Epoch 88/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9929 - acc: 0.8072 - val_loss: 0.9379 - val_acc: 0.8256\n",
      "epoch:88 learning rate:0.1\n",
      "Epoch 89/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9873 - acc: 0.8063 - val_loss: 1.0281 - val_acc: 0.7900\n",
      "epoch:89 learning rate:0.1\n",
      "Epoch 90/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9809 - acc: 0.8084 - val_loss: 0.9786 - val_acc: 0.8121\n",
      "epoch:90 learning rate:0.1\n",
      "Epoch 91/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9871 - acc: 0.8059 - val_loss: 1.1311 - val_acc: 0.7589\n",
      "epoch:91 learning rate:0.1\n",
      "Epoch 92/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9875 - acc: 0.8074 - val_loss: 0.9748 - val_acc: 0.8097\n",
      "epoch:92 learning rate:0.1\n",
      "Epoch 93/327\n",
      "1562/1562 [==============================] - 273s 174ms/step - loss: 0.9888 - acc: 0.8054 - val_loss: 0.9891 - val_acc: 0.7971\n",
      "epoch:93 learning rate:0.1\n",
      "Epoch 94/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9800 - acc: 0.8079 - val_loss: 1.0135 - val_acc: 0.7946\n",
      "epoch:94 learning rate:0.1\n",
      "Epoch 95/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9809 - acc: 0.8080 - val_loss: 1.0317 - val_acc: 0.7939\n",
      "epoch:95 learning rate:0.1\n",
      "Epoch 96/327\n",
      "1562/1562 [==============================] - 273s 174ms/step - loss: 0.9848 - acc: 0.8064 - val_loss: 1.0220 - val_acc: 0.7997\n",
      "epoch:96 learning rate:0.1\n",
      "Epoch 97/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9801 - acc: 0.8089 - val_loss: 0.9432 - val_acc: 0.8255\n",
      "epoch:97 learning rate:0.1\n",
      "Epoch 98/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.9896 - acc: 0.8071 - val_loss: 1.0372 - val_acc: 0.7944\n",
      "epoch:98 learning rate:0.1\n",
      "Epoch 99/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9863 - acc: 0.8090 - val_loss: 0.9572 - val_acc: 0.8201\n",
      "epoch:99 learning rate:0.1\n",
      "Epoch 100/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9761 - acc: 0.8097 - val_loss: 0.9761 - val_acc: 0.8150\n",
      "epoch:100 learning rate:0.1\n",
      "Epoch 101/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9816 - acc: 0.8082 - val_loss: 1.0894 - val_acc: 0.7779\n",
      "epoch:101 learning rate:0.1\n",
      "Epoch 102/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9735 - acc: 0.8099 - val_loss: 1.0333 - val_acc: 0.7919\n",
      "epoch:102 learning rate:0.1\n",
      "Epoch 103/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9804 - acc: 0.8080 - val_loss: 0.9335 - val_acc: 0.8258\n",
      "epoch:103 learning rate:0.1\n",
      "Epoch 104/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9713 - acc: 0.8129 - val_loss: 0.9990 - val_acc: 0.8076\n",
      "epoch:104 learning rate:0.1\n",
      "Epoch 105/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9782 - acc: 0.8131 - val_loss: 1.0327 - val_acc: 0.7973\n",
      "epoch:105 learning rate:0.1\n",
      "Epoch 106/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9779 - acc: 0.8096 - val_loss: 0.9544 - val_acc: 0.8171\n",
      "epoch:106 learning rate:0.1\n",
      "Epoch 107/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9806 - acc: 0.8092 - val_loss: 0.9956 - val_acc: 0.8013\n",
      "epoch:107 learning rate:0.1\n",
      "Epoch 108/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9766 - acc: 0.8090 - val_loss: 1.0390 - val_acc: 0.7897\n",
      "epoch:108 learning rate:0.1\n",
      "Epoch 109/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.9882 - acc: 0.8068 - val_loss: 1.0355 - val_acc: 0.7917\n",
      "epoch:109 learning rate:0.01\n",
      "Epoch 110/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.7493 - acc: 0.8881 - val_loss: 0.7753 - val_acc: 0.8725\n",
      "epoch:110 learning rate:0.01\n",
      "Epoch 111/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.6782 - acc: 0.9058 - val_loss: 0.7266 - val_acc: 0.8866\n",
      "epoch:111 learning rate:0.01\n",
      "Epoch 112/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.6438 - acc: 0.9121 - val_loss: 0.7134 - val_acc: 0.8828\n",
      "epoch:112 learning rate:0.01\n",
      "Epoch 113/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.6124 - acc: 0.9160 - val_loss: 0.6729 - val_acc: 0.8922\n",
      "epoch:113 learning rate:0.01\n",
      "Epoch 114/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.5882 - acc: 0.9187 - val_loss: 0.6808 - val_acc: 0.8849\n",
      "epoch:114 learning rate:0.01\n",
      "Epoch 115/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.5661 - acc: 0.9221 - val_loss: 0.6528 - val_acc: 0.8894\n",
      "epoch:115 learning rate:0.01\n",
      "Epoch 116/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.5470 - acc: 0.9235 - val_loss: 0.6297 - val_acc: 0.8929\n",
      "epoch:116 learning rate:0.01\n",
      "Epoch 117/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.5327 - acc: 0.9271 - val_loss: 0.6341 - val_acc: 0.8856\n",
      "epoch:117 learning rate:0.01\n",
      "Epoch 118/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.5183 - acc: 0.9268 - val_loss: 0.6149 - val_acc: 0.8899\n",
      "epoch:118 learning rate:0.01\n",
      "Epoch 119/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.5053 - acc: 0.9280 - val_loss: 0.6104 - val_acc: 0.8923\n",
      "epoch:119 learning rate:0.01\n",
      "Epoch 120/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4969 - acc: 0.9280 - val_loss: 0.6123 - val_acc: 0.8892\n",
      "epoch:120 learning rate:0.01\n",
      "Epoch 121/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4908 - acc: 0.9282 - val_loss: 0.5927 - val_acc: 0.8941\n",
      "epoch:121 learning rate:0.01\n",
      "Epoch 122/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4748 - acc: 0.9307 - val_loss: 0.5806 - val_acc: 0.8952\n",
      "epoch:122 learning rate:0.01\n",
      "Epoch 123/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4705 - acc: 0.9306 - val_loss: 0.5896 - val_acc: 0.8924\n",
      "epoch:123 learning rate:0.01\n",
      "Epoch 124/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4631 - acc: 0.9319 - val_loss: 0.6471 - val_acc: 0.8743\n",
      "epoch:124 learning rate:0.01\n",
      "Epoch 125/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4598 - acc: 0.9322 - val_loss: 0.5793 - val_acc: 0.8925\n",
      "epoch:125 learning rate:0.01\n",
      "Epoch 126/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4521 - acc: 0.9327 - val_loss: 0.5916 - val_acc: 0.8859\n",
      "epoch:126 learning rate:0.01\n",
      "Epoch 127/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4498 - acc: 0.9323 - val_loss: 0.5998 - val_acc: 0.8815\n",
      "epoch:127 learning rate:0.01\n",
      "Epoch 128/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4468 - acc: 0.9322 - val_loss: 0.5786 - val_acc: 0.8880\n",
      "epoch:128 learning rate:0.01\n",
      "Epoch 129/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4438 - acc: 0.9334 - val_loss: 0.5818 - val_acc: 0.8895\n",
      "epoch:129 learning rate:0.01\n",
      "Epoch 130/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4369 - acc: 0.9351 - val_loss: 0.5949 - val_acc: 0.8879\n",
      "epoch:130 learning rate:0.01\n",
      "Epoch 131/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4376 - acc: 0.9330 - val_loss: 0.5941 - val_acc: 0.8860\n",
      "epoch:131 learning rate:0.01\n",
      "Epoch 132/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4369 - acc: 0.9328 - val_loss: 0.5631 - val_acc: 0.8958\n",
      "epoch:132 learning rate:0.01\n",
      "Epoch 133/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4324 - acc: 0.9346 - val_loss: 0.5826 - val_acc: 0.8872\n",
      "epoch:133 learning rate:0.01\n",
      "Epoch 134/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4270 - acc: 0.9365 - val_loss: 0.5713 - val_acc: 0.8945\n",
      "epoch:134 learning rate:0.01\n",
      "Epoch 135/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4319 - acc: 0.9331 - val_loss: 0.6067 - val_acc: 0.8809\n",
      "epoch:135 learning rate:0.01\n",
      "Epoch 136/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4258 - acc: 0.9355 - val_loss: 0.5822 - val_acc: 0.8854\n",
      "epoch:136 learning rate:0.01\n",
      "Epoch 137/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4249 - acc: 0.9365 - val_loss: 0.5729 - val_acc: 0.8919\n",
      "epoch:137 learning rate:0.01\n",
      "Epoch 138/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4224 - acc: 0.9361 - val_loss: 0.5932 - val_acc: 0.8830\n",
      "epoch:138 learning rate:0.01\n",
      "Epoch 139/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4191 - acc: 0.9378 - val_loss: 0.5651 - val_acc: 0.8967\n",
      "epoch:139 learning rate:0.01\n",
      "Epoch 140/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4130 - acc: 0.9388 - val_loss: 0.5932 - val_acc: 0.8837\n",
      "epoch:140 learning rate:0.01\n",
      "Epoch 141/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4198 - acc: 0.9372 - val_loss: 0.5798 - val_acc: 0.8879\n",
      "epoch:141 learning rate:0.01\n",
      "Epoch 142/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4175 - acc: 0.9386 - val_loss: 0.5694 - val_acc: 0.8888\n",
      "epoch:142 learning rate:0.01\n",
      "Epoch 143/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4152 - acc: 0.9380 - val_loss: 0.5884 - val_acc: 0.8870\n",
      "epoch:143 learning rate:0.01\n",
      "Epoch 144/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4122 - acc: 0.9394 - val_loss: 0.5631 - val_acc: 0.8933\n",
      "epoch:144 learning rate:0.01\n",
      "Epoch 145/327\n",
      "1562/1562 [==============================] - 277s 178ms/step - loss: 0.4141 - acc: 0.9394 - val_loss: 0.5952 - val_acc: 0.8829\n",
      "epoch:145 learning rate:0.01\n",
      "Epoch 146/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.4135 - acc: 0.9406 - val_loss: 0.5979 - val_acc: 0.8843\n",
      "epoch:146 learning rate:0.01\n",
      "Epoch 147/327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 270s 173ms/step - loss: 0.4079 - acc: 0.9412 - val_loss: 0.5682 - val_acc: 0.8922\n",
      "epoch:147 learning rate:0.01\n",
      "Epoch 148/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4111 - acc: 0.9409 - val_loss: 0.5854 - val_acc: 0.8896\n",
      "epoch:148 learning rate:0.01\n",
      "Epoch 149/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4130 - acc: 0.9400 - val_loss: 0.5807 - val_acc: 0.8869\n",
      "epoch:149 learning rate:0.01\n",
      "Epoch 150/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4068 - acc: 0.9419 - val_loss: 0.5889 - val_acc: 0.8867\n",
      "epoch:150 learning rate:0.01\n",
      "Epoch 151/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4111 - acc: 0.9411 - val_loss: 0.5820 - val_acc: 0.8907\n",
      "epoch:151 learning rate:0.01\n",
      "Epoch 152/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4098 - acc: 0.9407 - val_loss: 0.5869 - val_acc: 0.8901\n",
      "epoch:152 learning rate:0.01\n",
      "Epoch 153/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4079 - acc: 0.9424 - val_loss: 0.6175 - val_acc: 0.8797\n",
      "epoch:153 learning rate:0.01\n",
      "Epoch 154/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4082 - acc: 0.9410 - val_loss: 0.6381 - val_acc: 0.8731\n",
      "epoch:154 learning rate:0.01\n",
      "Epoch 155/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4109 - acc: 0.9417 - val_loss: 0.6091 - val_acc: 0.8814\n",
      "epoch:155 learning rate:0.01\n",
      "Epoch 156/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4093 - acc: 0.9416 - val_loss: 0.5907 - val_acc: 0.8878\n",
      "epoch:156 learning rate:0.01\n",
      "Epoch 157/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4041 - acc: 0.9449 - val_loss: 0.7163 - val_acc: 0.8622\n",
      "epoch:157 learning rate:0.01\n",
      "Epoch 158/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4075 - acc: 0.9431 - val_loss: 0.6052 - val_acc: 0.8851\n",
      "epoch:158 learning rate:0.01\n",
      "Epoch 159/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4052 - acc: 0.9445 - val_loss: 0.5802 - val_acc: 0.8920\n",
      "epoch:159 learning rate:0.01\n",
      "Epoch 160/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4026 - acc: 0.9456 - val_loss: 0.5743 - val_acc: 0.8935\n",
      "epoch:160 learning rate:0.01\n",
      "Epoch 161/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4081 - acc: 0.9428 - val_loss: 0.5885 - val_acc: 0.8826\n",
      "epoch:161 learning rate:0.01\n",
      "Epoch 162/327\n",
      "1562/1562 [==============================] - 282s 180ms/step - loss: 0.4024 - acc: 0.9446 - val_loss: 0.6016 - val_acc: 0.8860\n",
      "epoch:162 learning rate:0.01\n",
      "Epoch 163/327\n",
      "1562/1562 [==============================] - 282s 180ms/step - loss: 0.3995 - acc: 0.9464 - val_loss: 0.6326 - val_acc: 0.8805\n",
      "epoch:163 learning rate:0.01\n",
      "Epoch 164/327\n",
      "1562/1562 [==============================] - 282s 180ms/step - loss: 0.4021 - acc: 0.9453 - val_loss: 0.5846 - val_acc: 0.8917\n",
      "epoch:164 learning rate:0.01\n",
      "Epoch 165/327\n",
      "1562/1562 [==============================] - 282s 180ms/step - loss: 0.3996 - acc: 0.9473 - val_loss: 0.5989 - val_acc: 0.8850\n",
      "epoch:165 learning rate:0.01\n",
      "Epoch 166/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4020 - acc: 0.9459 - val_loss: 0.5845 - val_acc: 0.8927\n",
      "epoch:166 learning rate:0.01\n",
      "Epoch 167/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.3999 - acc: 0.9471 - val_loss: 0.6130 - val_acc: 0.8837\n",
      "epoch:167 learning rate:0.01\n",
      "Epoch 168/327\n",
      "1562/1562 [==============================] - 282s 180ms/step - loss: 0.3980 - acc: 0.9465 - val_loss: 0.6144 - val_acc: 0.8855\n",
      "epoch:168 learning rate:0.01\n",
      "Epoch 169/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.3974 - acc: 0.9475 - val_loss: 0.6358 - val_acc: 0.8786\n",
      "epoch:169 learning rate:0.01\n",
      "Epoch 170/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4011 - acc: 0.9464 - val_loss: 0.6159 - val_acc: 0.8813\n",
      "epoch:170 learning rate:0.01\n",
      "Epoch 171/327\n",
      "1562/1562 [==============================] - 282s 181ms/step - loss: 0.4013 - acc: 0.9468 - val_loss: 0.5943 - val_acc: 0.8886\n",
      "epoch:171 learning rate:0.01\n",
      "Epoch 172/327\n",
      "1562/1562 [==============================] - 282s 180ms/step - loss: 0.3948 - acc: 0.9486 - val_loss: 0.6394 - val_acc: 0.8816\n",
      "epoch:172 learning rate:0.01\n",
      "Epoch 173/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.3950 - acc: 0.9481 - val_loss: 0.5959 - val_acc: 0.8928\n",
      "epoch:173 learning rate:0.01\n",
      "Epoch 174/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.4056 - acc: 0.9457 - val_loss: 0.6174 - val_acc: 0.8851\n",
      "epoch:174 learning rate:0.01\n",
      "Epoch 175/327\n",
      "1562/1562 [==============================] - 282s 180ms/step - loss: 0.3979 - acc: 0.9473 - val_loss: 0.6034 - val_acc: 0.8858\n",
      "epoch:175 learning rate:0.01\n",
      "Epoch 176/327\n",
      "1562/1562 [==============================] - 282s 180ms/step - loss: 0.3992 - acc: 0.9483 - val_loss: 0.6228 - val_acc: 0.8836\n",
      "epoch:176 learning rate:0.01\n",
      "Epoch 177/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.3972 - acc: 0.9481 - val_loss: 0.6163 - val_acc: 0.8843\n",
      "epoch:177 learning rate:0.01\n",
      "Epoch 178/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.3954 - acc: 0.9497 - val_loss: 0.6118 - val_acc: 0.8849\n",
      "epoch:178 learning rate:0.01\n",
      "Epoch 179/327\n",
      "1562/1562 [==============================] - 283s 181ms/step - loss: 0.4001 - acc: 0.9479 - val_loss: 0.6192 - val_acc: 0.8849\n",
      "epoch:179 learning rate:0.01\n",
      "Epoch 180/327\n",
      "1562/1562 [==============================] - 283s 181ms/step - loss: 0.3935 - acc: 0.9491 - val_loss: 0.6279 - val_acc: 0.8835\n",
      "epoch:180 learning rate:0.01\n",
      "Epoch 181/327\n",
      "1562/1562 [==============================] - 282s 180ms/step - loss: 0.4003 - acc: 0.9478 - val_loss: 0.6437 - val_acc: 0.8802\n",
      "epoch:181 learning rate:0.01\n",
      "Epoch 182/327\n",
      "1562/1562 [==============================] - 282s 180ms/step - loss: 0.3960 - acc: 0.9503 - val_loss: 0.6082 - val_acc: 0.8904\n",
      "epoch:182 learning rate:0.01\n",
      "Epoch 183/327\n",
      "1562/1562 [==============================] - 283s 181ms/step - loss: 0.3894 - acc: 0.9513 - val_loss: 0.6457 - val_acc: 0.8773\n",
      "epoch:183 learning rate:0.01\n",
      "Epoch 184/327\n",
      "1562/1562 [==============================] - 282s 181ms/step - loss: 0.3915 - acc: 0.9504 - val_loss: 0.6875 - val_acc: 0.8684\n",
      "epoch:184 learning rate:0.01\n",
      "Epoch 185/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.3939 - acc: 0.9505 - val_loss: 0.6152 - val_acc: 0.8865\n",
      "epoch:185 learning rate:0.01\n",
      "Epoch 186/327\n",
      "1562/1562 [==============================] - 282s 180ms/step - loss: 0.4005 - acc: 0.9476 - val_loss: 0.5865 - val_acc: 0.8945\n",
      "epoch:186 learning rate:0.01\n",
      "Epoch 187/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.3960 - acc: 0.9493 - val_loss: 0.6283 - val_acc: 0.8871\n",
      "epoch:187 learning rate:0.01\n",
      "Epoch 188/327\n",
      "1562/1562 [==============================] - 282s 180ms/step - loss: 0.3925 - acc: 0.9514 - val_loss: 0.6103 - val_acc: 0.8892\n",
      "epoch:188 learning rate:0.01\n",
      "Epoch 189/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.3968 - acc: 0.9485 - val_loss: 0.6225 - val_acc: 0.8869\n",
      "epoch:189 learning rate:0.01\n",
      "Epoch 190/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.3879 - acc: 0.9525 - val_loss: 0.6228 - val_acc: 0.8856\n",
      "epoch:190 learning rate:0.01\n",
      "Epoch 191/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.3925 - acc: 0.9509 - val_loss: 0.5986 - val_acc: 0.8943\n",
      "epoch:191 learning rate:0.01\n",
      "Epoch 192/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.3868 - acc: 0.9524 - val_loss: 0.6272 - val_acc: 0.8858\n",
      "epoch:192 learning rate:0.01\n",
      "Epoch 193/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.3893 - acc: 0.9525 - val_loss: 0.5819 - val_acc: 0.8926\n",
      "epoch:193 learning rate:0.01\n",
      "Epoch 194/327\n",
      "1562/1562 [==============================] - 281s 180ms/step - loss: 0.3928 - acc: 0.9508 - val_loss: 0.6113 - val_acc: 0.8849\n",
      "epoch:194 learning rate:0.01\n",
      "Epoch 195/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.3912 - acc: 0.9523 - val_loss: 0.6376 - val_acc: 0.8861\n",
      "epoch:195 learning rate:0.01\n",
      "Epoch 196/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.3952 - acc: 0.9497 - val_loss: 0.6329 - val_acc: 0.8856\n",
      "epoch:196 learning rate:0.01\n",
      "Epoch 197/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.3920 - acc: 0.9511 - val_loss: 0.6248 - val_acc: 0.8841\n",
      "epoch:197 learning rate:0.01\n",
      "Epoch 198/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.3990 - acc: 0.9493 - val_loss: 0.6409 - val_acc: 0.8768\n",
      "epoch:198 learning rate:0.01\n",
      "Epoch 199/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.3911 - acc: 0.9512 - val_loss: 0.6404 - val_acc: 0.8824\n",
      "epoch:199 learning rate:0.01\n",
      "Epoch 200/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.3941 - acc: 0.9497 - val_loss: 0.6042 - val_acc: 0.8911\n",
      "epoch:200 learning rate:0.01\n",
      "Epoch 201/327\n",
      "1562/1562 [==============================] - 273s 174ms/step - loss: 0.3913 - acc: 0.9520 - val_loss: 0.6152 - val_acc: 0.8870\n",
      "epoch:201 learning rate:0.01\n",
      "Epoch 202/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3910 - acc: 0.9533 - val_loss: 0.6365 - val_acc: 0.8844\n",
      "epoch:202 learning rate:0.01\n",
      "Epoch 203/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3868 - acc: 0.9539 - val_loss: 0.6802 - val_acc: 0.8739\n",
      "epoch:203 learning rate:0.01\n",
      "Epoch 204/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3914 - acc: 0.9523 - val_loss: 0.6483 - val_acc: 0.8750\n",
      "epoch:204 learning rate:0.01\n",
      "Epoch 205/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3849 - acc: 0.9547 - val_loss: 0.6081 - val_acc: 0.8908\n",
      "epoch:205 learning rate:0.01\n",
      "Epoch 206/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3920 - acc: 0.9513 - val_loss: 0.6555 - val_acc: 0.8797\n",
      "epoch:206 learning rate:0.01\n",
      "Epoch 207/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3936 - acc: 0.9514 - val_loss: 0.7077 - val_acc: 0.8674\n",
      "epoch:207 learning rate:0.01\n",
      "Epoch 208/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3862 - acc: 0.9541 - val_loss: 0.6173 - val_acc: 0.8888\n",
      "epoch:208 learning rate:0.01\n",
      "Epoch 209/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3912 - acc: 0.9527 - val_loss: 0.6366 - val_acc: 0.8846\n",
      "epoch:209 learning rate:0.01\n",
      "Epoch 210/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3851 - acc: 0.9550 - val_loss: 0.6480 - val_acc: 0.8834\n",
      "epoch:210 learning rate:0.01\n",
      "Epoch 211/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3875 - acc: 0.9544 - val_loss: 0.6238 - val_acc: 0.8866\n",
      "epoch:211 learning rate:0.01\n",
      "Epoch 212/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3898 - acc: 0.9523 - val_loss: 0.5835 - val_acc: 0.8949\n",
      "epoch:212 learning rate:0.01\n",
      "Epoch 213/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3886 - acc: 0.9537 - val_loss: 0.6404 - val_acc: 0.8882\n",
      "epoch:213 learning rate:0.01\n",
      "Epoch 214/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3897 - acc: 0.9526 - val_loss: 0.6915 - val_acc: 0.8735\n",
      "epoch:214 learning rate:0.01\n",
      "Epoch 215/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3959 - acc: 0.9510 - val_loss: 0.5984 - val_acc: 0.8930\n",
      "epoch:215 learning rate:0.01\n",
      "Epoch 216/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3868 - acc: 0.9539 - val_loss: 0.6435 - val_acc: 0.8825\n",
      "epoch:216 learning rate:0.01\n",
      "Epoch 217/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3867 - acc: 0.9538 - val_loss: 0.6786 - val_acc: 0.8773\n",
      "epoch:217 learning rate:0.01\n",
      "Epoch 218/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3940 - acc: 0.9522 - val_loss: 0.5983 - val_acc: 0.8906\n",
      "epoch:218 learning rate:0.001\n",
      "Epoch 219/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.3135 - acc: 0.9840 - val_loss: 0.5529 - val_acc: 0.9080\n",
      "epoch:219 learning rate:0.001\n",
      "Epoch 220/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2974 - acc: 0.9895 - val_loss: 0.5568 - val_acc: 0.9102\n",
      "epoch:220 learning rate:0.001\n",
      "Epoch 221/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2927 - acc: 0.9898 - val_loss: 0.5522 - val_acc: 0.9103\n",
      "epoch:221 learning rate:0.001\n",
      "Epoch 222/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2878 - acc: 0.9909 - val_loss: 0.5594 - val_acc: 0.9112\n",
      "epoch:222 learning rate:0.001\n",
      "Epoch 223/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2859 - acc: 0.9919 - val_loss: 0.5579 - val_acc: 0.9109\n",
      "epoch:223 learning rate:0.001\n",
      "Epoch 224/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2834 - acc: 0.9918 - val_loss: 0.5576 - val_acc: 0.9110\n",
      "epoch:224 learning rate:0.001\n",
      "Epoch 225/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2808 - acc: 0.9926 - val_loss: 0.5584 - val_acc: 0.9102\n",
      "epoch:225 learning rate:0.001\n",
      "Epoch 226/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2778 - acc: 0.9930 - val_loss: 0.5565 - val_acc: 0.9119\n",
      "epoch:226 learning rate:0.001\n",
      "Epoch 227/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2767 - acc: 0.9935 - val_loss: 0.5582 - val_acc: 0.9113\n",
      "epoch:227 learning rate:0.001\n",
      "Epoch 228/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2755 - acc: 0.9930 - val_loss: 0.5564 - val_acc: 0.9100\n",
      "epoch:228 learning rate:0.001\n",
      "Epoch 229/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2729 - acc: 0.9936 - val_loss: 0.5537 - val_acc: 0.9109\n",
      "epoch:229 learning rate:0.001\n",
      "Epoch 230/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2706 - acc: 0.9944 - val_loss: 0.5596 - val_acc: 0.9079\n",
      "epoch:230 learning rate:0.001\n",
      "Epoch 231/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2698 - acc: 0.9939 - val_loss: 0.5616 - val_acc: 0.9103\n",
      "epoch:231 learning rate:0.001\n",
      "Epoch 232/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2700 - acc: 0.9938 - val_loss: 0.5543 - val_acc: 0.9112\n",
      "epoch:232 learning rate:0.001\n",
      "Epoch 233/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2677 - acc: 0.9944 - val_loss: 0.5567 - val_acc: 0.9097\n",
      "epoch:233 learning rate:0.001\n",
      "Epoch 234/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2660 - acc: 0.9941 - val_loss: 0.5565 - val_acc: 0.9095\n",
      "epoch:234 learning rate:0.001\n",
      "Epoch 235/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2641 - acc: 0.9946 - val_loss: 0.5570 - val_acc: 0.9075\n",
      "epoch:235 learning rate:0.001\n",
      "Epoch 236/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2628 - acc: 0.9946 - val_loss: 0.5543 - val_acc: 0.9111\n",
      "epoch:236 learning rate:0.001\n",
      "Epoch 237/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2616 - acc: 0.9946 - val_loss: 0.5540 - val_acc: 0.9121\n",
      "epoch:237 learning rate:0.001\n",
      "Epoch 238/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2603 - acc: 0.9950 - val_loss: 0.5498 - val_acc: 0.9114\n",
      "epoch:238 learning rate:0.001\n",
      "Epoch 239/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2585 - acc: 0.9953 - val_loss: 0.5540 - val_acc: 0.9101\n",
      "epoch:239 learning rate:0.001\n",
      "Epoch 240/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2568 - acc: 0.9954 - val_loss: 0.5545 - val_acc: 0.9111\n",
      "epoch:240 learning rate:0.001\n",
      "Epoch 241/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2562 - acc: 0.9952 - val_loss: 0.5500 - val_acc: 0.9095\n",
      "epoch:241 learning rate:0.001\n",
      "Epoch 242/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2553 - acc: 0.9954 - val_loss: 0.5567 - val_acc: 0.9104\n",
      "epoch:242 learning rate:0.001\n",
      "Epoch 243/327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.2535 - acc: 0.9957 - val_loss: 0.5573 - val_acc: 0.9117\n",
      "epoch:243 learning rate:0.001\n",
      "Epoch 244/327\n",
      "1562/1562 [==============================] - 272s 174ms/step - loss: 0.2530 - acc: 0.9952 - val_loss: 0.5491 - val_acc: 0.9117\n",
      "epoch:244 learning rate:0.001\n",
      "Epoch 245/327\n",
      "1562/1562 [==============================] - 273s 174ms/step - loss: 0.2524 - acc: 0.9956 - val_loss: 0.5482 - val_acc: 0.9101\n",
      "epoch:245 learning rate:0.001\n",
      "Epoch 246/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2497 - acc: 0.9961 - val_loss: 0.5475 - val_acc: 0.9122\n",
      "epoch:246 learning rate:0.001\n",
      "Epoch 247/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2490 - acc: 0.9955 - val_loss: 0.5469 - val_acc: 0.9120\n",
      "epoch:247 learning rate:0.001\n",
      "Epoch 248/327\n",
      "1562/1562 [==============================] - 273s 174ms/step - loss: 0.2485 - acc: 0.9956 - val_loss: 0.5512 - val_acc: 0.9105\n",
      "epoch:248 learning rate:0.001\n",
      "Epoch 249/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2481 - acc: 0.9954 - val_loss: 0.5469 - val_acc: 0.9119\n",
      "epoch:249 learning rate:0.001\n",
      "Epoch 250/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2466 - acc: 0.9958 - val_loss: 0.5454 - val_acc: 0.9114\n",
      "epoch:250 learning rate:0.001\n",
      "Epoch 251/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2450 - acc: 0.9962 - val_loss: 0.5490 - val_acc: 0.9106\n",
      "epoch:251 learning rate:0.001\n",
      "Epoch 252/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2450 - acc: 0.9959 - val_loss: 0.5466 - val_acc: 0.9113\n",
      "epoch:252 learning rate:0.001\n",
      "Epoch 253/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2431 - acc: 0.9962 - val_loss: 0.5437 - val_acc: 0.9106\n",
      "epoch:253 learning rate:0.001\n",
      "Epoch 254/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2425 - acc: 0.9960 - val_loss: 0.5511 - val_acc: 0.9104\n",
      "epoch:254 learning rate:0.001\n",
      "Epoch 255/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2413 - acc: 0.9961 - val_loss: 0.5452 - val_acc: 0.9095\n",
      "epoch:255 learning rate:0.001\n",
      "Epoch 256/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2407 - acc: 0.9962 - val_loss: 0.5460 - val_acc: 0.9121\n",
      "epoch:256 learning rate:0.001\n",
      "Epoch 257/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2398 - acc: 0.9962 - val_loss: 0.5504 - val_acc: 0.9102\n",
      "epoch:257 learning rate:0.001\n",
      "Epoch 258/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2387 - acc: 0.9961 - val_loss: 0.5449 - val_acc: 0.9089\n",
      "epoch:258 learning rate:0.001\n",
      "Epoch 259/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2378 - acc: 0.9962 - val_loss: 0.5452 - val_acc: 0.9100\n",
      "epoch:259 learning rate:0.001\n",
      "Epoch 260/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2371 - acc: 0.9964 - val_loss: 0.5428 - val_acc: 0.9110\n",
      "epoch:260 learning rate:0.001\n",
      "Epoch 261/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2359 - acc: 0.9962 - val_loss: 0.5437 - val_acc: 0.9084\n",
      "epoch:261 learning rate:0.001\n",
      "Epoch 262/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2350 - acc: 0.9962 - val_loss: 0.5410 - val_acc: 0.9101\n",
      "epoch:262 learning rate:0.001\n",
      "Epoch 263/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2345 - acc: 0.9959 - val_loss: 0.5424 - val_acc: 0.9091\n",
      "epoch:263 learning rate:0.001\n",
      "Epoch 264/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2332 - acc: 0.9965 - val_loss: 0.5334 - val_acc: 0.9114\n",
      "epoch:264 learning rate:0.001\n",
      "Epoch 265/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2320 - acc: 0.9964 - val_loss: 0.5370 - val_acc: 0.9089\n",
      "epoch:265 learning rate:0.001\n",
      "Epoch 266/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2304 - acc: 0.9971 - val_loss: 0.5502 - val_acc: 0.9094\n",
      "epoch:266 learning rate:0.001\n",
      "Epoch 267/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2306 - acc: 0.9965 - val_loss: 0.5400 - val_acc: 0.9106\n",
      "epoch:267 learning rate:0.001\n",
      "Epoch 268/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2298 - acc: 0.9964 - val_loss: 0.5397 - val_acc: 0.9114\n",
      "epoch:268 learning rate:0.001\n",
      "Epoch 269/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2290 - acc: 0.9965 - val_loss: 0.5421 - val_acc: 0.9081\n",
      "epoch:269 learning rate:0.001\n",
      "Epoch 270/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2278 - acc: 0.9967 - val_loss: 0.5360 - val_acc: 0.9117\n",
      "epoch:270 learning rate:0.001\n",
      "Epoch 271/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2273 - acc: 0.9966 - val_loss: 0.5378 - val_acc: 0.9104\n",
      "epoch:271 learning rate:0.001\n",
      "Epoch 272/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2277 - acc: 0.9961 - val_loss: 0.5344 - val_acc: 0.9108\n",
      "epoch:272 learning rate:0.001\n",
      "Epoch 273/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2268 - acc: 0.9963 - val_loss: 0.5332 - val_acc: 0.9106\n",
      "epoch:273 learning rate:0.001\n",
      "Epoch 274/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2253 - acc: 0.9965 - val_loss: 0.5315 - val_acc: 0.9112\n",
      "epoch:274 learning rate:0.001\n",
      "Epoch 275/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2248 - acc: 0.9964 - val_loss: 0.5312 - val_acc: 0.9090\n",
      "epoch:275 learning rate:0.001\n",
      "Epoch 276/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2238 - acc: 0.9965 - val_loss: 0.5318 - val_acc: 0.9113\n",
      "epoch:276 learning rate:0.001\n",
      "Epoch 277/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2230 - acc: 0.9966 - val_loss: 0.5312 - val_acc: 0.9112\n",
      "epoch:277 learning rate:0.001\n",
      "Epoch 278/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2223 - acc: 0.9969 - val_loss: 0.5299 - val_acc: 0.9100\n",
      "epoch:278 learning rate:0.001\n",
      "Epoch 279/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2211 - acc: 0.9973 - val_loss: 0.5254 - val_acc: 0.9117\n",
      "epoch:279 learning rate:0.001\n",
      "Epoch 280/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2210 - acc: 0.9971 - val_loss: 0.5384 - val_acc: 0.9081\n",
      "epoch:280 learning rate:0.001\n",
      "Epoch 281/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2203 - acc: 0.9966 - val_loss: 0.5299 - val_acc: 0.9079\n",
      "epoch:281 learning rate:0.001\n",
      "Epoch 282/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2199 - acc: 0.9964 - val_loss: 0.5335 - val_acc: 0.9072\n",
      "epoch:282 learning rate:0.001\n",
      "Epoch 283/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2181 - acc: 0.9971 - val_loss: 0.5330 - val_acc: 0.9077\n",
      "epoch:283 learning rate:0.001\n",
      "Epoch 284/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2175 - acc: 0.9971 - val_loss: 0.5348 - val_acc: 0.9083\n",
      "epoch:284 learning rate:0.001\n",
      "Epoch 285/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2172 - acc: 0.9967 - val_loss: 0.5270 - val_acc: 0.9096\n",
      "epoch:285 learning rate:0.001\n",
      "Epoch 286/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2167 - acc: 0.9965 - val_loss: 0.5232 - val_acc: 0.9106\n",
      "epoch:286 learning rate:0.001\n",
      "Epoch 287/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2159 - acc: 0.9966 - val_loss: 0.5227 - val_acc: 0.9104\n",
      "epoch:287 learning rate:0.001\n",
      "Epoch 288/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2145 - acc: 0.9972 - val_loss: 0.5221 - val_acc: 0.9094\n",
      "epoch:288 learning rate:0.001\n",
      "Epoch 289/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2140 - acc: 0.9970 - val_loss: 0.5253 - val_acc: 0.9110\n",
      "epoch:289 learning rate:0.001\n",
      "Epoch 290/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2144 - acc: 0.9965 - val_loss: 0.5227 - val_acc: 0.9097\n",
      "epoch:290 learning rate:0.001\n",
      "Epoch 291/327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2132 - acc: 0.9971 - val_loss: 0.5231 - val_acc: 0.9098\n",
      "epoch:291 learning rate:0.001\n",
      "Epoch 292/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2134 - acc: 0.9964 - val_loss: 0.5261 - val_acc: 0.9088\n",
      "epoch:292 learning rate:0.001\n",
      "Epoch 293/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2124 - acc: 0.9968 - val_loss: 0.5322 - val_acc: 0.9063\n",
      "epoch:293 learning rate:0.001\n",
      "Epoch 294/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2110 - acc: 0.9970 - val_loss: 0.5364 - val_acc: 0.9050\n",
      "epoch:294 learning rate:0.001\n",
      "Epoch 295/327\n",
      "1562/1562 [==============================] - 275s 176ms/step - loss: 0.2109 - acc: 0.9968 - val_loss: 0.5309 - val_acc: 0.9065\n",
      "epoch:295 learning rate:0.001\n",
      "Epoch 296/327\n",
      "1562/1562 [==============================] - 276s 177ms/step - loss: 0.2106 - acc: 0.9966 - val_loss: 0.5175 - val_acc: 0.9103\n",
      "epoch:296 learning rate:0.001\n",
      "Epoch 297/327\n",
      "1562/1562 [==============================] - 277s 178ms/step - loss: 0.2097 - acc: 0.9967 - val_loss: 0.5258 - val_acc: 0.9090\n",
      "epoch:297 learning rate:0.001\n",
      "Epoch 298/327\n",
      "1562/1562 [==============================] - 277s 177ms/step - loss: 0.2099 - acc: 0.9968 - val_loss: 0.5276 - val_acc: 0.9069\n",
      "epoch:298 learning rate:0.001\n",
      "Epoch 299/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2084 - acc: 0.9971 - val_loss: 0.5253 - val_acc: 0.9079\n",
      "epoch:299 learning rate:0.001\n",
      "Epoch 300/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2081 - acc: 0.9969 - val_loss: 0.5319 - val_acc: 0.9061\n",
      "epoch:300 learning rate:0.001\n",
      "Epoch 301/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2083 - acc: 0.9965 - val_loss: 0.5242 - val_acc: 0.9078\n",
      "epoch:301 learning rate:0.001\n",
      "Epoch 302/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2067 - acc: 0.9968 - val_loss: 0.5231 - val_acc: 0.9083\n",
      "epoch:302 learning rate:0.001\n",
      "Epoch 303/327\n",
      "1562/1562 [==============================] - 273s 175ms/step - loss: 0.2062 - acc: 0.9969 - val_loss: 0.5164 - val_acc: 0.9091\n",
      "epoch:303 learning rate:0.001\n",
      "Epoch 304/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2063 - acc: 0.9968 - val_loss: 0.5238 - val_acc: 0.9077\n",
      "epoch:304 learning rate:0.001\n",
      "Epoch 305/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2051 - acc: 0.9972 - val_loss: 0.5153 - val_acc: 0.9086\n",
      "epoch:305 learning rate:0.001\n",
      "Epoch 306/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2043 - acc: 0.9970 - val_loss: 0.5260 - val_acc: 0.9047\n",
      "epoch:306 learning rate:0.001\n",
      "Epoch 307/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2050 - acc: 0.9968 - val_loss: 0.5235 - val_acc: 0.9064\n",
      "epoch:307 learning rate:0.001\n",
      "Epoch 308/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2033 - acc: 0.9969 - val_loss: 0.5202 - val_acc: 0.9078\n",
      "epoch:308 learning rate:0.001\n",
      "Epoch 309/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2032 - acc: 0.9969 - val_loss: 0.5235 - val_acc: 0.9070\n",
      "epoch:309 learning rate:0.001\n",
      "Epoch 310/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2022 - acc: 0.9969 - val_loss: 0.5175 - val_acc: 0.9084\n",
      "epoch:310 learning rate:0.001\n",
      "Epoch 311/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2019 - acc: 0.9967 - val_loss: 0.5126 - val_acc: 0.9085\n",
      "epoch:311 learning rate:0.001\n",
      "Epoch 312/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2016 - acc: 0.9972 - val_loss: 0.5166 - val_acc: 0.9081\n",
      "epoch:312 learning rate:0.001\n",
      "Epoch 313/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2003 - acc: 0.9971 - val_loss: 0.5185 - val_acc: 0.9072\n",
      "epoch:313 learning rate:0.001\n",
      "Epoch 314/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.2003 - acc: 0.9971 - val_loss: 0.5148 - val_acc: 0.9076\n",
      "epoch:314 learning rate:0.001\n",
      "Epoch 315/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.1998 - acc: 0.9969 - val_loss: 0.5155 - val_acc: 0.9088\n",
      "epoch:315 learning rate:0.001\n",
      "Epoch 316/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.1989 - acc: 0.9971 - val_loss: 0.5185 - val_acc: 0.9057\n",
      "epoch:316 learning rate:0.001\n",
      "Epoch 317/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.1984 - acc: 0.9972 - val_loss: 0.5116 - val_acc: 0.9073\n",
      "epoch:317 learning rate:0.001\n",
      "Epoch 318/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.1988 - acc: 0.9968 - val_loss: 0.5277 - val_acc: 0.9043\n",
      "epoch:318 learning rate:0.001\n",
      "Epoch 319/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.1986 - acc: 0.9968 - val_loss: 0.5145 - val_acc: 0.9068\n",
      "epoch:319 learning rate:0.001\n",
      "Epoch 320/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.1979 - acc: 0.9968 - val_loss: 0.5095 - val_acc: 0.9055\n",
      "epoch:320 learning rate:0.001\n",
      "Epoch 321/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.1967 - acc: 0.9969 - val_loss: 0.5144 - val_acc: 0.9073\n",
      "epoch:321 learning rate:0.001\n",
      "Epoch 322/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.1970 - acc: 0.9966 - val_loss: 0.5102 - val_acc: 0.9060\n",
      "epoch:322 learning rate:0.001\n",
      "Epoch 323/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.1954 - acc: 0.9972 - val_loss: 0.5228 - val_acc: 0.9061\n",
      "epoch:323 learning rate:0.001\n",
      "Epoch 324/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.1962 - acc: 0.9966 - val_loss: 0.5111 - val_acc: 0.9047\n",
      "epoch:324 learning rate:0.001\n",
      "Epoch 325/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.1947 - acc: 0.9972 - val_loss: 0.5135 - val_acc: 0.9089\n",
      "epoch:325 learning rate:0.001\n",
      "Epoch 326/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.1949 - acc: 0.9968 - val_loss: 0.5312 - val_acc: 0.9011\n",
      "epoch:326 learning rate:0.001\n",
      "Epoch 327/327\n",
      "1562/1562 [==============================] - 274s 175ms/step - loss: 0.1952 - acc: 0.9963 - val_loss: 0.5280 - val_acc: 0.9037\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "                    steps_per_epoch=steps_in_epoch,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    epochs=epochs, verbose=1, max_q_size=10,\n",
    "                    callbacks=[lr_scheduler, csv_logger])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
